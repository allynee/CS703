cvxpy version:
Version: 1.1.24
License: Apache License, Version 2.0
enum34 version:
Version: 1.1.10
h5py version:
Version: 3.1.0
numpy version:
Version: 1.19.5
torchnet version:
Version: 0.0.4
tqdm version:
Version: 4.64.1
You are allocated NVIDIA GeForce RTX 3090 on lava
You are using GPU 3
Loading FC100 dataset - phase train
Loading FC100 dataset - phase val
using gpu: 0, 1, 2, 3
{'num_epoch': 60, 'save_epoch': 10, 'train_shot': 15, 'val_shot': 5, 'train_query': 6, 'val_episode': 2000, 'val_query': 15, 'train_way': 5, 'test_way': 5, 'save_path': './experiments/FC100_MetaOptNet_RR', 'gpu': '0, 1, 2, 3', 'network': 'ResNet', 'head': 'Ridge', 'dataset': 'FC100', 'episodes_per_batch': 8, 'eps': 0.0}
Train Epoch: 1	Learning Rate: 0.1000
/common/home/projectgrps/CS704/CS704G1/.conda/envs/metalearning/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
n_support: 75 n_way: 5 n_shot: 15
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:03<00:35,  3.98s/it] 20%|██        | 2/10 [00:04<00:14,  1.77s/it] 30%|███       | 3/10 [00:04<00:07,  1.07s/it] 40%|████      | 4/10 [00:04<00:04,  1.34it/s] 50%|█████     | 5/10 [00:04<00:02,  1.79it/s] 60%|██████    | 6/10 [00:05<00:01,  2.20it/s] 70%|███████   | 7/10 [00:05<00:01,  2.66it/s] 80%|████████  | 8/10 [00:05<00:00,  2.97it/s] 90%|█████████ | 9/10 [00:05<00:00,  3.28it/s]100%|██████████| 10/10 [00:06<00:00,  3.53it/s]100%|██████████| 10/10 [00:06<00:00,  1.59it/s]
n_support: 25 n_way: 5 n_shot: 15
  0%|          | 0/2000 [00:00<?, ?it/s]  0%|          | 0/2000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 250, in <module>
    logit_query = cls_head.forward(cls_head.head, cls_head.scale, cls_head.enable_scale, emb_query, emb_support, labels_support, opt.train_way, opt.train_shot)
  File "/common/home/projectgrps/CS704/CS704G1/MetaOptNet/models/classification_heads.py", line 560, in forward
    return scale * head(query, support, support_labels, n_way, n_shot, **kwargs)
  File "/common/home/projectgrps/CS704/CS704G1/MetaOptNet/models/classification_heads.py", line 90, in MetaOptNetHead_Ridge
    assert(n_support == n_way * n_shot)      # n_support must equal to n_way * n_shot
AssertionError
srun: error: lava: task 0: Exited with exit code 1
srun: Terminating StepId=48305.1
Job ID: 48305
Cluster: crimson
User/Group: CS704G1/CS704G1
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:33
CPU Efficiency: 51.56% of 00:01:04 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 3.47 MB
Memory Efficiency: 0.02% of 16.00 GB
